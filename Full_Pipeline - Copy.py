{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d2906d0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import nltk \n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "from scipy.special import softmax\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6f31829a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter a Comment please : At best, this can be mid weight, but that is a stretch.  I have been looking for a heavyweight sweatshirt and thought this would be it.  Back to the drawing board....\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c767cd062c994251af9f725171c18487",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "******************** Gradient Boosting Classifier model **********\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment</th>\n",
       "      <th>roberta_neg</th>\n",
       "      <th>predected</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>At best, this can be mid weight, but that is a...</td>\n",
       "      <td>0.316578</td>\n",
       "      <td>material issue</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             comment roberta_neg  \\\n",
       "1  At best, this can be mid weight, but that is a...    0.316578   \n",
       "\n",
       "        predected  \n",
       "1  material issue  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "co = [input(\"Enter a Comment please : \")]\n",
    "def full_pipeline(co):\n",
    "    # Import pre-trained model\n",
    "    MODEL = f\"cardiffnlp/twitter-roberta-base-sentiment\" \n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL)\n",
    "\n",
    "    # Create an object \n",
    "    model = AutoModelForSequenceClassification.from_pretrained(MODEL)\n",
    "    \n",
    "    \n",
    "    def polarity_scores_roberta(example):\n",
    "        # Tokenize the comments\n",
    "        encoded_text = tokenizer(example, return_tensors='pt')\n",
    "\n",
    "        # input to the model \n",
    "        output = model(**encoded_text) \n",
    "        scores = output[0][0].detach().numpy()\n",
    "        scores = softmax(scores)\n",
    "\n",
    "         # create a new columns with names\n",
    "        scores_dict = {\n",
    "            'roberta_neg' : scores[0],\n",
    "            'roberta_neu' : scores[1],\n",
    "            'roberta_pos' : scores[2],\n",
    "            'comment' : ' '.join(co)\n",
    "        }\n",
    "\n",
    "        #return a diect with the polarity of the comments  \n",
    "        return scores_dict \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    sia = SentimentIntensityAnalyzer()\n",
    "    res = {}\n",
    "    for i in tqdm(co, total=len(co)):\n",
    "        try:\n",
    "            text = str(co)\n",
    "            myid = len(co)\n",
    "            vader_result = sia.polarity_scores(text)\n",
    "            vader_result_rename = {}\n",
    "            for key, value in vader_result.items():\n",
    "                vader_result_rename[f\"vader_{key}\"] = value\n",
    "            roberta_result = polarity_scores_roberta(text)\n",
    "            both = {**vader_result_rename, **roberta_result}\n",
    "            res[myid] = both\n",
    "        except RuntimeError:\n",
    "            print(f'Broke for id {myid}')\n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "      \n",
    "    \n",
    "    res = pd.DataFrame(res).T\n",
    "    neg_df = res[(res['roberta_neg'] > res['roberta_pos'])] \n",
    "    neg_df = neg_df[['comment', 'roberta_neg']]\n",
    "    if len(neg_df) == 0:\n",
    "        print('Sorry, the comment eather positive nor neutral !! ')\n",
    "        \n",
    "    else:\n",
    "        #function to input the cleaning function to multi-core processing\n",
    "        def clean_apply(df):\n",
    "            from nltk.tokenize import word_tokenize\n",
    "            import re\n",
    "            import spacy\n",
    "            import string\n",
    "            from nltk.stem import PorterStemmer, SnowballStemmer, LancasterStemmer, WordNetLemmatizer\n",
    "            from nltk.corpus import stopwords\n",
    "            global stop_words\n",
    "            stop_words = [\"a\",\"able\",\"about\",\"above\",\"abst\",\"accordance\",\"according\",\"accordingly\",\"across\",\"act\",\"actually\",\"added\",\n",
    "                      \"adj\",\"affected\",\"affecting\",\"affects\",\"after\",\"afterwards\",\"again\",\"against\",\"ah\",\"all\",\"almost\",\"alone\",\n",
    "                      \"along\",\"already\",\"also\",\"although\",\"always\",\"am\",\"among\",\"amongst\",\"an\",\"and\",\"announce\",\"another\",\"any\",\n",
    "                      \"anybody\",\"anyhow\",\"anymore\",\"anyone\",\"anything\",\"anyway\",\"anyways\",\"anywhere\",\"apparently\",\"approximately\",\n",
    "                      \"are\",\"aren\",\"arent\",\"arise\",\"around\",\"as\",\"aside\",\"ask\",\"asking\",\"at\",\"auth\",\"available\",\"away\",\"awfully\",\n",
    "                      \"b\",\"back\",\"be\",\"became\",\"because\",\"become\",\"becomes\",\"becoming\",\"been\",\"before\",\"beforehand\",\"begin\",\n",
    "                      \"beginning\",\"beginnings\",\"begins\",\"behind\",\"being\",\"believe\",\"below\",\"beside\",\"besides\",\"between\",\"beyond\",\n",
    "                      \"biol\",\"both\",\"brief\",\"briefly\",\"but\",\"by\",\"c\",\"ca\",\"came\",\"can\",\"cannot\",\"cause\",\"causes\",\"certain\",\n",
    "                      \"certainly\",\"co\",\"com\",\"come\",\"comes\",\"contain\",\"containing\",\"contains\",\"could\",\"couldnt\",\"d\",\"date\",\"did\",\n",
    "                      \"different\",\"do\",\"does\",\"doing\",\"done\",\"down\",\"downwards\",\"due\",\"during\",\"e\",\"each\",\"ed\",\"edu\",\"effect\",\n",
    "                      \"eg\",\"eight\",\"eighty\",\"either\",\"else\",\"elsewhere\",\"end\",\"ending\",\"enough\",\"especially\",\"et\",\"etc\",\"even\",\n",
    "                      \"ever\",\"every\",\"everybody\",\"everyone\",\"everything\",\"everywhere\",\"ex\",\"except\",\"f\",\"far\",\"few\",\"ff\",\"fifth\",\n",
    "                      \"first\",\"five\",\"fix\",\"followed\",\"following\",\"follows\",\"for\",\"former\",\"formerly\",\"forth\",\"found\",\"four\",\"from\",\n",
    "                      \"further\",\"furthermore\",\"g\",\"gave\",\"get\",\"gets\",\"getting\",\"give\",\"given\",\"gives\",\"giving\",\"go\",\"goes\",\"gone\",\n",
    "                      \"got\",\"gotten\",\"h\",\"had\",\"happens\",\"hardly\",\"has\",\"have\",\"having\",\"he\",\"hed\",\"hence\",\"her\",\"here\",\"hereafter\",\"hereby\",\"herein\",\"heres\",\"hereupon\",\"hers\",\"herself\",\"hes\",\"hi\",\"hid\",\"him\",\"himself\",\"his\",\"hither\",\"home\",\"how\",\"howbeit\",\"however\",\"hundred\",\"i\",\"id\",\"ie\",\"if\",\"im\",\"immediate\",\"immediately\",\"importance\",\"important\",\"in\",\"inc\",\"indeed\",\"index\",\"information\",\"instead\",\"into\",\"invention\",\"inward\",\"is\",\"it\",\"itd\",\"its\",\"itself\",\"j\",\"just\",\"k\",\"keep\",\"keeps\",\"kept\",\"kg\",\"km\",\"know\",\"known\",\"knows\",\"l\",\"largely\",\"last\",\"lately\",\"later\",\"latter\",\"latterly\",\"least\",\"less\",\"lest\",\"let\",\"lets\",\"like\",\"liked\",\"likely\",\"line\",\"little\",\"look\",\"looking\",\"looks\",\"ltd\",\"m\",\"made\",\"mainly\",\"make\",\"makes\",\"many\",\"may\",\"maybe\",\"me\",\"mean\",\"means\",\"meantime\",\"meanwhile\",\"merely\",\"mg\",\"might\",\"million\",\"miss\",\"ml\",\"more\",\"moreover\",\"most\",\"mostly\",\"mr\",\"mrs\",\"much\",\"mug\",\"must\",\"my\",\"myself\",\"n\",\"na\",\"name\",\"namely\",\"nay\",\"nd\",\"near\",\"nearly\",\"necessarily\",\"necessary\",\"need\",\"needs\",\"neither\",\"never\",\"nevertheless\",\"new\",\"next\",\"nine\",\"ninety\",\"no\",\"nobody\",\"non\",\"none\",\"nonetheless\",\"noone\",\"nor\",\"normally\",\"nos\",\"not\",\"noted\",\"nothing\",\"now\",\"nowhere\",\"o\",\"obtain\",\"obtained\",\"obviously\",\"of\",\"off\",\"often\",\"oh\",\"ok\",\"okay\",\"old\",\"omitted\",\"on\",\"once\",\"one\",\"ones\",\"only\",\"onto\",\"or\",\"ord\",\"other\",\"others\",\"otherwise\",\"ought\",\"our\",\"ours\",\"ourselves\",\"out\",\"outside\",\"over\",\"overall\",\"owing\",\"own\",\"p\",\"page\",\"pages\",\"part\",\"particular\",\"particularly\",\"past\",\"per\",\"perhaps\",\"placed\",\"please\",\"plus\",\"poorly\",\"possible\",\"possibly\",\"potentially\",\"pp\",\"predominantly\",\"present\",\"previously\",\"primarily\",\"probably\",\"promptly\",\"proud\",\"provides\",\"put\",\"q\",\"que\",\"quickly\",\"quite\",\"qv\",\"r\",\"ran\",\"rather\",\"rd\",\"re\",\"readily\",\"really\",\"recent\",\"recently\",\"ref\",\"refs\",\"regarding\",\"regardless\",\"regards\",\"related\",\"relatively\",\"research\",\"respectively\",\"resulted\",\"resulting\",\"results\",\"right\",\"run\",\"s\",\"said\",\"same\",\"saw\",\"say\",\"saying\",\"says\",\"sec\",\"section\",\"see\",\"seeing\",\"seem\",\"seemed\",\"seeming\",\"seems\",\"seen\",\"self\",\"selves\",\"sent\",\"seven\",\"several\",\"shall\",\"she\",\"shed\",\"shes\",\"should\",\"show\",\"showed\",\"shown\",\"showns\",\"shows\",\"significant\",\"significantly\",\n",
    "                       \"similar\",\"similarly\",\"since\",\"six\",\"slightly\",\"so\",\"some\",\"somebody\",\"somehow\",\"someone\",\"somethan\",\n",
    "                      \"something\",\"sometime\",\"sometimes\",\"somewhat\",\"somewhere\",\"soon\",\"sorry\",\"specifically\",\"specified\",\"specify\",\n",
    "                      \"specifying\",\"still\",\"stop\",\"strongly\",\"sub\",\"substantially\",\"successfully\",\"such\",\"sufficiently\",\"suggest\",\n",
    "                      \"sup\",\"sure\",\"t\",\"take\",\"taken\",\"taking\",\"tell\",\"tends\",\"th\",\"than\",\"thank\",\"thanks\",\"thanx\",\"that\",\"thats\",\n",
    "                      \"the\",\"their\",\"theirs\",\"them\",\"themselves\",\"then\",\"thence\",\"there\",\"thereafter\",\"thereby\",\"thered\",\"therefore\",\n",
    "                      \"therein\",\"thereof\",\"therere\",\"theres\",\"thereto\",\"thereupon\",\"these\",\"they\",\"theyd\",\"theyre\",\"think\",\"this\",\n",
    "                      \"those\",\"thou\",\"though\",\"thoughh\",\"thousand\",\"throug\",\"through\",\"throughout\",\"thru\",\"thus\",\"til\",\"tip\",\"to\",\n",
    "                      \"together\",\"too\",\"took\",\"toward\",\"towards\",\"tried\",\"tries\",\"truly\",\"try\",\"trying\",\"ts\",\"twice\",\"two\",\"u\",\"un\",\n",
    "                      \"under\",\"unfortunately\",\"unless\",\"unlike\",\"unlikely\",\"until\",\"unto\",\"up\",\"upon\",\"ups\",\"us\",\"use\",\"used\",\n",
    "                      \"useful\",\"usefully\",\"usefulness\",\"uses\",\"using\",\"usually\",\"v\",\"value\",\"various\",\"very\",\"via\",\"viz\",\"vol\",\n",
    "                      \"vols\",\"vs\",\"w\",\"want\",\"wants\",\"was\",\"wasnt\",\"way\",\"we\",\"wed\",\"welcome\",\"went\",\"were\",\"werent\",\"we've\",\"what\",\n",
    "                      \"whatever\",\"whats\",\"when\",\"whence\",\"whenever\",\"where\",\"whereafter\",\"whereas\",\"whereby\",\"wherein\",\"wheres\",\n",
    "                      \"whereupon\",\"wherever\",\"whether\",\"which\",\"while\",\"whim\",\"whither\",\"who\",\"whod\",\"whoever\",\"whole\",\"whom\",\n",
    "                      \"whomever\",\"whos\",\"whose\",\"why\",\"widely\",\"willing\",\"wish\",\"with\",\"within\",\"without\",\"wont\",\"words\",\"world\",\n",
    "                      \"would\",\"wouldnt\",\"www\",\"x\",\"y\",\"yes\",\"yet\",\"you\",\"youd\",\"your\",\"youre\",\"yours\",\"yourself\",\"yourselves\",\"z\",\n",
    "                      \"zero\"'i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\",\n",
    "                      'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself',\n",
    "                      'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this',\n",
    "                      'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does',\n",
    "                      'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against',\n",
    "                      'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again',\n",
    "                      'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such',\n",
    "                      'own', 'same', 'so', 'than', 'love', 'like', 'loved', 'good', 'great', 'soft', 'super', 'perfect', 'comfy', 'nice', 'quality', 'true', \n",
    "                      'cute', 'comfortable','l', 'xl', 'xxl', 'warm', 'perfectly', 'expected', 'fit', 'pretty', 'perfect', 'sweatshirt', 'buy', 'bought', 'lb']\n",
    "        #     stop_words = stopwords.words(\"english\")\n",
    "        #     stop_words = stop_words.extend(['not', 'Not','aren','aren\\'t','couldn','couldn\\'t','didn','didn\\'t','doesn','doesn\\'t','hadn','hadn\\'t','hasn','hasn\\'t','haven','haven\\'t','isn','isn\\'t','ma','mightn','mightn\\'t','mustn','mustn\\'t','needn','needn\\'t','shan',\"shan't\",'shouldn',\"shouldn't\",'wasn',\"wasn't\",'weren',\"weren't\",'won',\"won't\",'wouldn',\"wouldn't\"])\n",
    "\n",
    "            nlp = spacy.load('en_core_web_sm', disable=['parser', 'ner'])\n",
    "\n",
    "\n",
    "            def clean_text(text):\n",
    "                text = text.lower() #lower-casing\n",
    "                text = re.sub(r'[^\\w\\s]', ' ', text)\n",
    "                text = re.sub(r'\\d+',' ',text) #removing numbers\n",
    "                text = text.translate(str.maketrans('','',string.punctuation)) #removing punctuation\n",
    "                text = text.lower() #lower-casing\n",
    "                text = [i for i in word_tokenize(text) if i not in stop_words] #remvoving stop-words\n",
    "        #         doc = nlp(' '.join(text))\n",
    "                stemmer = WordNetLemmatizer()\n",
    "                text = [stemmer.lemmatize(token) for token in text] #lemmatizing the reviews\n",
    "                text = ' '.join(text)    \n",
    "                text = text.strip() #removing white-spaces\n",
    "                return text\n",
    "            df['comment'] = df.comment.apply(clean_text)\n",
    "            return df\n",
    "\n",
    "\n",
    "\n",
    "        clean_apply(neg_df)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        df = pd.read_csv('/anaconda/JupyterNotebooK/DataSets/neg_df_labeled.csv', index_col=0)\n",
    "        df = df.fillna('').reset_index(drop=True)\n",
    "\n",
    "        X = df['comment']\n",
    "        y = df['pred_cat']\n",
    "\n",
    "        x = neg_df['comment']\n",
    "\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "        vectorizer = TfidfVectorizer()\n",
    "        X_train = vectorizer.fit_transform(X_train)\n",
    "        x = vectorizer.transform(x)\n",
    "\n",
    "\n",
    "        import joblib\n",
    "        # joblib.dump(lo, 'LogisticRegression_model.sav')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # load the model from disk\n",
    "#         loaded_model = joblib.load('LogisticRegression_model.sav')\n",
    "#         predected = loaded_model.predict(x)\n",
    "#         # result = loaded_model.score(x, y_test)\n",
    "#         # print(result)\n",
    "#         neg_df['predected'] = predected\n",
    "#         neg_df['comment'] = co\n",
    "#         neg_df.drop('roberta_neg', axis = 1, inplace = True)\n",
    "#         print('*'*20 , 'Logistic Regression model', '*'*20)\n",
    "#         display(neg_df)\n",
    "        \n",
    "        \n",
    "        # load the model from disk\n",
    "        loaded_model = joblib.load('GradientBoostingClassifier_model.sav')\n",
    "        predected = loaded_model.predict(x)\n",
    "#         result = loaded_model.score(x, y_test)\n",
    "#         print(result)\n",
    "        neg_df['predected'] = predected\n",
    "        neg_df['comment'] = co\n",
    "        print('')\n",
    "        print('*'*20 , 'Gradient Boosting Classifier model', '*'*10)\n",
    "        display(neg_df)\n",
    "\n",
    "full_pipeline(co)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d35977c0",
   "metadata": {},
   "source": [
    "# Done\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "ec3b6344b8b3f6c3db1463248c46b8920f4fc68f6187e2c1e71f56a52299e245"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
